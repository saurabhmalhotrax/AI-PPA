# MVP Prompts for AI-Assisted Development

## Sprint 0: Environment & Repo Setup

### Task 0.1: Initialize Git repository
**Context:** This is the first step in setting up our project. We need a version control system to track changes.
**Objective:** Initialize a new Git repository in the current project's root directory.
**Key Files/Modules:** Project root.
**Detailed Steps/Requirements:**
1.  Execute the command to initialize a Git repository.
2.  Verify that a `.git` directory has been created.
**Deliverables:**
*   A confirmed message that the Git repository has been initialized successfully.
*   Presence of a `.git` hidden folder in the project root.
**Rules/Best Practices:** Ensure the command is run in the correct root directory of the project.
---

### Task 0.2: Create a Python virtual environment
**Context:** To manage project-specific dependencies and avoid conflicts with global Python packages, we need a virtual environment.
**Objective:** Create a Python virtual environment named `venv` in the project's root directory using Python 3.9+.
**Key Files/Modules:** Project root.
**Detailed Steps/Requirements:**
1.  Ensure Python 3.9 or a later compatible version is available.
2.  Execute the command to create a virtual environment named `venv`. For example: `python3 -m venv venv`.
3.  Verify that a `venv` directory has been created.
**Deliverables:**
*   A confirmed message that the virtual environment `venv` has been created.
*   Presence of a `venv` folder in the project root.
**Rules/Best Practices:** The virtual environment should typically not be committed to Git (it will be covered by `.gitignore`).
---

### Task 0.3: Generate .gitignore file
**Context:** We need to prevent unnecessary or sensitive files from being committed to the Git repository.
**Objective:** Create a `.gitignore` file in the project root, populated with common Python and environment-specific patterns to ignore.
**Key Files/Modules:** `.gitignore` (to be created).
**Detailed Steps/Requirements:**
1.  Create a new file named `.gitignore` in the project root.
2.  Add the following patterns to the file:
    ```
    # Virtual Environment
    venv/
    *.pyc
    __pycache__/
    .env

    # IDE specific
    .vscode/
    .idea/

    # OS specific
    .DS_Store
    Thumbs.db

    # Build artifacts
    build/
    dist/
    *.egg-info/

    # Data files (if large or sensitive, or generated)
    data/extracted_invoices.csv # Example, adjust if needed
    data/duplicate_pairs_ground_truth.csv # Example
    data/contracts.csv # Example
    *.pt # PyTorch models
    *.png # Generated images like gnn_embeddings.png
    ```
**Deliverables:**
*   A `.gitignore` file created in the project root with the specified content.
**Rules/Best Practices:** Ensure the patterns cover common files generated by Python projects, IDEs, OS, and our specific data/model outputs that shouldn't be versioned.
---

### Task 0.4: Create requirements.txt file
**Context:** This file will list all Python dependencies required for the project, ensuring a reproducible environment.
**Objective:** Create an empty `requirements.txt` file in the project root. It will be populated in the next task.
**Key Files/Modules:** `requirements.txt` (to be created).
**Detailed Steps/Requirements:**
1.  Create a new file named `requirements.txt` in the project root.
**Deliverables:**
*   An empty `requirements.txt` file created in the project root.
---

### Task 0.5: Add initial core dependencies to requirements.txt
**Context:** We need to list the foundational Python libraries for our MVP.
**Objective:** Add specified core Python libraries to the `requirements.txt` file.
**Key Files/Modules:** `requirements.txt`.
**Detailed Steps/Requirements:**
1.  Open the `requirements.txt` file.
2.  Add the following lines, specifying versions known to be compatible or the latest stable versions if unsure (AI can help suggest current stable versions if needed, but these are good starting points):
    ```
    streamlit==1.30.0
    pandas==2.1.0
    requests==2.31.0
    python-dotenv==1.0.0
    # Add versions for any OS-specific dependencies if needed, e.g., for faiss later
    ```
**Deliverables:**
*   The `requirements.txt` file updated with `streamlit`, `pandas`, `requests`, and `python-dotenv`.
**Rules/Best Practices:** It's good practice to pin versions for reproducibility. The AI can help find the latest stable versions at the time of development.
---

### Task 0.6: Create docker-compose.yml for Neo4j
**Context:** We need a local Neo4j instance for graph database functionalities. Docker Compose simplifies managing this service.
**Objective:** Create a `docker-compose.yml` file to define and configure a Neo4j service.
**Key Files/Modules:** `docker-compose.yml` (to be created).
**Detailed Steps/Requirements:**
1.  Create a new file named `docker-compose.yml` in the project root.
2.  Define the Neo4j service as per Sub-tasks 0.6.1, 0.6.2, 0.6.3 from the MVP plan:
    ```yaml
    version: '3.8' # Or a recent stable version

    services:
      neo4j:
        image: neo4j:4.4 # MVP plan specified 4.4, can use 5.x if preferred but ensure py2neo compatibility
        container_name: mvp_neo4j
        ports:
          - "7474:7474" # HTTP
          - "7687:7687" # Bolt
        volumes:
          - mvp_neo4j_data:/data # Persist data
          # - ./neo4j/plugins:/plugins # If custom plugins are needed later
        environment:
          - NEO4J_AUTH=neo4j/testpassword # Change 'testpassword' to something more secure if desired, even for local
          - NEO4J_dbms_memory_heap_initial__size=512m
          - NEO4J_dbms_memory_heap_max__size=2G # As per MVP plan
          # NEO4J_ACCEPT_LICENSE_AGREEMENT=yes # Required for Enterprise Edition, not strictly for Community
    volumes:
      mvp_neo4j_data:
    ```
**Deliverables:**
*   A `docker-compose.yml` file created in the project root with the specified Neo4j service definition.
**Rules/Best Practices:** Use a specific Neo4j version tag (e.g., `neo4j:4.4.29-community` or `neo4j:5.17.0-community`) for stability. Ensure `py2neo` version chosen later is compatible. Using named volumes for data persistence is good practice.
---

### Task 0.7: Create Dockerfile for the main application
**Context:** We need to containerize our main Python application (Streamlit/Flask) for consistency and ease of deployment, even locally.
**Objective:** Create a `Dockerfile` for the Python application.
**Key Files/Modules:** `Dockerfile` (to be created).
**Detailed Steps/Requirements:**
1.  Create a new file named `Dockerfile` in the project root.
2.  Define the Docker image build steps as per Sub-tasks 0.7.1, 0.7.2, 0.7.3 from the MVP plan:
    ```dockerfile
    # Base image
    FROM python:3.9-slim

    # Set working directory
    WORKDIR /app

    # Install system dependencies if any are identified later (e.g., for specific libraries)
    # RUN apt-get update && apt-get install -y --no-install-recommends some-package && rm -rf /var/lib/apt/lists/*

    # Copy requirements file and install dependencies
    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    # Copy application code
    COPY . .
    # If src layout is used strictly:
    # COPY src/ ./src/
    # COPY app.py .
    # COPY scripts/ ./scripts/


    # Expose port (Streamlit default is 8501)
    EXPOSE 8501

    # Command to run the application (update if using Flask or other entrypoint)
    # For development, you might mount code as a volume in docker-compose and run directly.
    # For a built image, this would be the run command.
    CMD ["streamlit", "run", "app.py"]
    ```
**Deliverables:**
*   A `Dockerfile` file created in the project root with the specified content.
**Rules/Best Practices:** Start with a slim base image. Install dependencies before copying application code to leverage Docker layer caching. Specify an entry point/command.
---

### Task 0.8: Create app.py (main Streamlit app file) with a "Hello World" title
**Context:** This is the entry point for our Streamlit user interface.
**Objective:** Create a basic `app.py` file that imports Streamlit and displays a title.
**Key Files/Modules:** `app.py` (to be created).
**Detailed Steps/Requirements:**
1.  Create a new file named `app.py` in the project root.
2.  Add the following Python code:
    ```python
    import streamlit as st

    def main():
        st.set_page_config(page_title="MVP Invoice Auditor", layout="wide")
        st.title("Invoice Auditing MVP - Hello World!")
        st.write("Welcome to the AI-assisted invoice auditing system.")

        # Placeholder for future content
        # st.sidebar.header("Navigation")
        # page = st.sidebar.radio("Go to", ["Upload & Extract", "View Duplicates", "Compliance Check"])

        # if page == "Upload & Extract":
        #     st.header("Upload and Extract Invoice Data")
            # ... upload logic here ...

    if __name__ == "__main__":
        main()
    ```
**Deliverables:**
*   An `app.py` file created in the project root with the basic Streamlit "Hello World" setup.
**Rules/Best Practices:** Structure the Streamlit app within a `main()` function. Use `st.set_page_config()` early.
---

### Task 0.9: Create src/ directory for modules
**Context:** To organize our codebase, we will place reusable Python modules into an `src` directory.
**Objective:** Create a directory named `src` in the project root.
**Key Files/Modules:** `src/` (directory to be created).
**Detailed Steps/Requirements:**
1.  Create a new directory named `src` in the project root.
2.  (Optional but good practice) Create an empty `src/__init__.py` file to mark it as a package.
**Deliverables:**
*   An `src` directory created in the project root.
*   (Optional) An `src/__init__.py` file.
---

### Task 0.10: Create src/config.py for API keys and settings
**Context:** We need a centralized place to manage configuration, especially sensitive API keys, which will be loaded from environment variables.
**Objective:** Create `src/config.py` to load and provide access to configuration settings from environment variables.
**Key Files/Modules:** `src/config.py` (to be created).
**Detailed Steps/Requirements:**
1.  Create a new file named `src/config.py`.
2.  Add Python code to load environment variables using `dotenv` and `os`:
    ```python
    import os
    from dotenv import load_dotenv

    # Load environment variables from .env file
    load_dotenv()

    # Vision AI API Key (OpenAI or Gemini)
    # Ensure you set this in your .env file
    VISION_API_KEY = os.getenv("VISION_API_KEY")
    # Example: OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    # Example: GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

    # Neo4j Configuration (if needed directly in code, though often handled by py2neo connect string)
    NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
    NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "testpassword") # Match docker-compose

    # Add other configurations as needed
    # Example: EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"

    # Basic check for essential keys
    if not VISION_API_KEY:
        print("Warning: VISION_API_KEY not found in environment variables. Please set it in your .env file.")

    ```
**Deliverables:**
*   A `src/config.py` file created with logic to load specified environment variables.
**Rules/Best Practices:** Never hardcode API keys. Load them from environment variables. `python-dotenv` is useful for local development with `.env` files.
---

### Task 0.11: Create .env.example file
**Context:** To guide users (and our future selves) on what environment variables are needed for the project to run.
**Objective:** Create an `.env.example` file listing required environment variables with placeholder values.
**Key Files/Modules:** `.env.example` (to be created).
**Detailed Steps/Requirements:**
1.  Create a new file named `.env.example` in the project root.
2.  Add the following content:
    ```env
    # Vision AI API Key (choose one and rename or use a generic VISION_API_KEY in config.py)
    OPENAI_API_KEY="YOUR_OPENAI_API_KEY_HERE"
    # GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE"
    VISION_API_KEY="YOUR_ACTUAL_VISION_API_KEY_HERE" # If using a generic key in config.py

    # Neo4j (if overriding defaults in config.py or for external connection)
    # NEO4J_URI="bolt://localhost:7687"
    # NEO4J_USER="neo4j"
    # NEO4J_PASSWORD="testpassword" # Should match your docker-compose or actual Neo4j setup
    ```
**Deliverables:**
*   An `.env.example` file created in the project root with the specified template.
**Rules/Best Practices:** This file should be committed to Git. Developers will copy it to `.env` (which is gitignored) and fill in their actual credentials.
---

### Task 0.12: Add black and flake8 to requirements.txt
**Context:** For maintaining code quality and consistency through automated formatting and linting.
**Objective:** Add `black` and `flake8` to the `requirements.txt` file.
**Key Files/Modules:** `requirements.txt`.
**Detailed Steps/Requirements:**
1.  Open `requirements.txt`.
2.  Append the following lines:
    ```
    # Linters and Formatters
    black==23.10.0
    flake8==6.1.0
    ```
**Deliverables:**
*   `requirements.txt` updated with `black` and `flake8`.
**Rules/Best Practices:** Pinning linter versions helps ensure consistent behavior across environments.
---

### Task 0.13: Configure pyproject.toml or setup.cfg for linters
**Context:** To define specific rules and configurations for `black` and `flake8`.
**Objective:** Create or update `pyproject.toml` (preferred for `black`) or `setup.cfg` to configure the linters.
**Key Files/Modules:** `pyproject.toml` or `setup.cfg`.
**Detailed Steps/Requirements:**
1.  Create `pyproject.toml` in the project root if it doesn't exist.
2.  Add configuration for `black` and `flake8`. Example for `pyproject.toml`:
    ```toml
    [tool.black]
    line-length = 88
    target-version = ['py39'] # Adjust to your Python version
    # include = '\\.pyi?$'
    # exclude = '''
    # /(
    #     \\.git
    #   | \\.hg
    #   | \\.mypy_cache
    #   | \\.tox
    #   | \\.venv
    #   | _build
    #   | buck-out
    #   | build
    #   | dist
    # )/
    # '''

    [tool.flake8]
    # Or in setup.cfg as [flake8]
    ignore = ['E203', 'E266', 'E501', 'W503'] # E501 is line length, black handles it. W503 is line break before binary operator.
    max-line-length = 88 # Should match black
    max-complexity = 10
    exclude = [
        ".git",
        "__pycache__",
        "docs/source/conf.py",
        "old",
        "build",
        "dist",
        ".venv",
        "venv"
    ]
    ```
**Deliverables:**
*   A `pyproject.toml` or `setup.cfg` file with linter configurations.
**Rules/Best Practices:** `pyproject.toml` is the modern standard for Python project configuration. Align `max-line-length` between `black` and `flake8`. Customize `ignore` codes for `flake8` as per project preferences, but start with a sensible set.
---

## Sprint 1: Vision API Integration & Data Extraction

### Task 1.1: Create Vision API Client (`src/vision_extractor.py`)
**Context:** We need a reusable module to interact with the chosen Vision AI API (OpenAI GPT-4o Vision or Google Gemini 1.5 Pro Vision) to extract structured data from invoice images.
**Objective:** Implement a Python function `extract_invoice_data(image_bytes, api_key)` in `src/vision_extractor.py` that takes image bytes and an API key, calls the Vision API, and returns a structured dictionary of extracted fields.
**Key Files/Modules:** `src/vision_extractor.py`, `src/config.py` (for API key).
**Detailed Steps/Requirements:**
1.  **File Creation:** Create `src/vision_extractor.py` if it doesn't exist.
2.  **Import Necessary Libraries:** `requests` (for HTTP calls if not using a dedicated client library), `base64` (if sending image bytes as base64 string), `json`.
3.  **Import API Key:** Import `VISION_API_KEY` from `src.config`.
4.  **Implement `extract_invoice_data(image_bytes: bytes, use_openai: bool = True)` function:**
    *   **Parameter `use_openai`**: Add a boolean flag to select between OpenAI and Gemini (or other future models). Default to one.
    *   **Image Encoding (if required by API):** If the API expects a base64 encoded string, encode `image_bytes`.
    *   **Prompt Definition (Sub-task 1.1.2):**
        *   Create a clear, detailed prompt specifying the desired output format (JSON) and the exact fields to extract: `invoice_no` (string), `date` (string, ideally YYYY-MM-DD), `vendor` (string), `total_amount` (float or string that can be easily converted to float).
        *   Example prompt structure (adapt for chosen API):
            ```json
            {
              "model": "gpt-4o", // or Gemini model
              "messages": [
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "Extract the following fields from this invoice image and return them as a valid JSON object: invoice_no (invoice number or ID), date (invoice date in YYYY-MM-DD format if possible, otherwise as seen), vendor (supplier or vendor name), total_amount (the final total amount due). If a field is not found, use null for its value."
                    },
                    {
                      "type": "image_url",
                      "image_url": {
                        "url": "data:image/jpeg;base64,{base64_encoded_image_bytes}"
                      }
                    }
                  ]
                }
              ],
              "max_tokens": 300,
              "temperature": 0.2 // For more deterministic output
            }
            ```
    *   **API Call (Sub-task 1.1.1):**
        *   Construct the appropriate HTTP request (headers, payload) for the chosen Vision API (OpenAI or Gemini).
        *   Use the `requests` library to make the POST request.
    *   **Response Parsing (Sub-task 1.1.3):**
        *   Check the HTTP response status code.
        *   If successful, parse the JSON response from the API.
        *   Extract the part of the response containing the structured data (often a JSON string within the main JSON response that needs to be parsed again).
        *   Convert the extracted data into a Python dictionary matching the requested fields: `{"invoice_no": "...", "date": "...", "vendor": "...", "total_amount": ...}`.
    *   **Error Handling (Sub-task 1.1.4):**
        *   Implement try-except blocks for network errors, API errors (e.g., bad status codes, rate limits), and JSON parsing errors.
        *   Log errors appropriately (e.g., using the `logging` module).
        *   Return `None` or raise a custom exception if extraction fails.
5.  **Helper for data type conversion (Optional):** After parsing, attempt to convert `total_amount` to float and `date` to a standard date string or object if needed.

**Deliverables:**
*   The `src/vision_extractor.py` file containing the `extract_invoice_data` function with the specified logic.
*   Function should be callable and tested (unit tests in Task 1.4).
**Rules/Best Practices:**
*   Make the function modular and testable.
*   Clearly document the expected input image format (bytes) and the output dictionary structure.
*   Handle API keys securely (loaded from config, not hardcoded).
*   Provide informative error messages.
---

### Task 1.2: Create Script to Load RVL-CDIP Samples (`scripts/load_rvl_cdip_samples.py`)
**Context:** We need a consistent set of sample invoice images for development and testing the extraction process.
**Objective:** Implement a Python script to download/access approximately 200 invoice images from the RVL-CDIP dataset and save them locally.
**Key Files/Modules:** `scripts/load_rvl_cdip_samples.py`, `data/sample_invoices/` (directory to be created).
**Detailed Steps/Requirements:**
1.  **File Creation:** Create `scripts/load_rvl_cdip_samples.py`.
2.  **Import Libraries:** `datasets` (from Hugging Face), `os`, `PIL` (Pillow) for image handling.
3.  **Add `datasets` to `requirements.txt`** if not already present.
4.  **Define Output Directory:** `output_dir = "data/sample_invoices"`.
5.  **Create Output Directory:** Use `os.makedirs(output_dir, exist_ok=True)`.
6.  **Load RVL-CDIP Dataset (Sub-task 1.2.1):**
    *   Use `load_dataset("aharley/rvl-cdip", split='train', streaming=True)` for memory efficiency or a portion of a specific split.
    *   Iterate through the dataset.
7.  **Filter for Invoices and Select Samples:**
    *   Filter examples where `label == 11` (invoice class for RVL-CDIP).
    *   Collect up to 200 such samples.
    *   To ensure variety and manage download size if not streaming, you might take a slice or use `dataset.take()` if streaming.
8.  **Save Images (Sub-task 1.2.2):**
    *   For each selected invoice sample, get the image data (usually a `PIL.Image` object).
    *   Construct a unique filename (e.g., `rvl_cdip_invoice_{i}.png` or use an ID from the dataset if available).
    *   Save the image to the `output_dir` using `image.save(filepath)`.
    *   Log progress (e.g., "Saved image X of 200").
9.  **Script Execution:** Ensure the script is runnable from the command line.

**Deliverables:**
*   The `scripts/load_rvl_cdip_samples.py` script.
*   A directory `data/sample_invoices/` populated with ~200 invoice images after running the script.
**Rules/Best Practices:**
*   Handle potential errors during dataset loading or image saving.
*   Make the number of samples configurable if possible.
*   Ensure the script is idempotent (i.e., running it again doesn't cause issues if images already exist, perhaps by checking first or overwriting).
---

### Task 1.3: Create Batch Extraction Script (`scripts/batch_extract.py`)
**Context:** After obtaining sample images, we need a script to process them in batch using our vision extractor and save the structured results.
**Objective:** Implement `scripts/batch_extract.py` to iterate through local invoice images, call the `extract_invoice_data` function for each, and save all results to a CSV file.
**Key Files/Modules:** `scripts/batch_extract.py`, `src/vision_extractor.py`, `src/config.py`, `data/sample_invoices/`, `data/extracted_invoices.csv` (to be created).
**Detailed Steps/Requirements:**
1.  **File Creation:** Create `scripts/batch_extract.py`.
2.  **Import Libraries:** `os`, `pandas`, `logging`.
3.  **Import Custom Modules:** `from src.vision_extractor import extract_invoice_data`, `from src.config import VISION_API_KEY`.
4.  **Configure Logging (Sub-task 1.3.5):** Set up basic logging to track progress and errors (e.g., log to console and/or a file).
5.  **Define Input/Output Paths:**
    *   `input_image_dir = "data/sample_invoices/"`
    *   `output_csv_file = "data/extracted_invoices.csv"`
6.  **Iterate Through Images (Sub-task 1.3.1):**
    *   Use `os.listdir()` and `os.path.join()` to get paths of all image files in `input_image_dir`.
    *   Filter for common image extensions (e.g., `.png`, `.jpg`, `.jpeg`).
7.  **Process Each Image (Sub-task 1.3.2):**
    *   For each image file:
        *   Open the image file in binary read mode (`rb`) and read its content (`image_bytes`).
        *   Call `extracted_info = extract_invoice_data(image_bytes, VISION_API_KEY)` (or pass the specific API key if `config.py` gives options).
        *   Log success or failure for each image.
        *   If successful and `extracted_info` is not None, add the `filename` to `extracted_info`.
        *   Append `extracted_info` to a list `all_extracted_data` (Sub-task 1.3.3).
8.  **Save Results to CSV (Sub-task 1.3.4):**
    *   If `all_extracted_data` is not empty, convert it to a Pandas DataFrame.
    *   Ensure columns are consistent (e.g., `filename`, `invoice_no`, `date`, `vendor`, `total_amount`).
    *   Save the DataFrame to `output_csv_file` using `df.to_csv(index=False)`.
9.  **Script Execution:** Make the script runnable from the command line.

**Deliverables:**
*   The `scripts/batch_extract.py` script.
*   A `data/extracted_invoices.csv` file generated by running the script, containing structured data for the processed invoices.
**Rules/Best Practices:**
*   Handle cases where `extract_invoice_data` returns `None` (e.g., log the error and skip the image).
*   Include a progress indicator if processing many files (e.g., print current file number).
*   Ensure the CSV has a header row and consistent columns.
---

### Task 1.4 & 1.5: Create Unit Tests for Vision Extractor (`tests/test_vision_extractor.py`)
**Context:** To ensure the reliability and correctness of our vision extraction logic, we need unit tests.
**Objective:** Implement unit tests for the `extract_invoice_data` function in `src/vision_extractor.py`, including tests for successful extraction and error handling, using mocking for API calls.
**Key Files/Modules:** `tests/test_vision_extractor.py` (to be created), `src/vision_extractor.py`, `unittest` or `pytest` framework.
**Detailed Steps/Requirements:**
1.  **Test File Creation:** Create `tests/test_vision_extractor.py`.
2.  **Add `pytest` and `pytest-mock` (or just `pytest` if using `unittest.mock`) to `requirements.txt` if not present.**
3.  **Import Libraries:** `pytest` (or `unittest`), `unittest.mock.patch` (or `pytest_mock.mocker`).
4.  **Import Target Function:** `from src.vision_extractor import extract_invoice_data`.
5.  **Prepare Sample Data:**
    *   Have a sample image file (e.g., `tests/fixtures/sample_invoice.png`) or mock image bytes.
    *   Prepare a mock successful API JSON response string that `extract_invoice_data` would expect to parse.
    *   Prepare a mock error API JSON response or an exception to be raised by the mock.
6.  **Test Case for Successful Extraction (Sub-task 1.5.1):**
    *   Define a test function (e.g., `test_extract_invoice_data_success`).
    *   Use `@patch('src.vision_extractor.requests.post')` (if `requests` is directly used) or mock the specific API client library call if one is used.
    *   Configure the mock object to return a mock response object that has a `.status_code = 200` and a `.json()` method returning the parsed mock successful API JSON response.
    *   Call `extract_invoice_data()` with sample image bytes and a dummy API key.
    *   Assert that the returned dictionary matches the expected structured data (e.g., correct keys, values, and types).
7.  **Test Case for API Error Handling (Sub-task 1.5.2):**
    *   Define a test function (e.g., `test_extract_invoice_data_api_error`).
    *   Use `@patch` similarly.
    *   Configure the mock `requests.post` to return a response with an error status code (e.g., 400 or 500) or to raise an exception (e.g., `requests.exceptions.RequestException`).
    *   Call `extract_invoice_data()`.
    *   Assert that the function handles the error gracefully (e.g., returns `None` or raises the expected custom exception, depending on its design).
8.  **Test Case for JSON Parsing Error (Optional but good):**
    *   Define a test function.
    *   Mock the API call to return a successful status code but with malformed JSON in the response content.
    *   Assert that `extract_invoice_data()` handles this (e.g., returns `None` or logs an error).
9.  **Run Tests:** Ensure tests can be discovered and run by `pytest` (e.g., `pytest tests/`).

**Deliverables:**
*   The `tests/test_vision_extractor.py` file with implemented unit tests covering successful extraction and error scenarios.
*   All tests passing when run with `pytest`.
**Rules/Best Practices:**
*   Tests should be independent and not rely on live API calls.
*   Mock external dependencies thoroughly.
*   Cover both successful paths and common failure modes.
*   Use clear assertion messages.
--- 

## Sprint 2: Duplicate Detection with FAISS

### Task 2.1: Add FAISS & Sentence-Transformers to `requirements.txt`
**Context:** We need libraries for generating text embeddings and performing efficient similarity search.
**Objective:** Update `requirements.txt` to include `sentence-transformers` and `faiss-cpu` (or `faiss-gpu`).
**Key Files/Modules:** `requirements.txt`.
**Detailed Steps/Requirements:**
1.  Open `requirements.txt`.
2.  Append the following lines, choosing the appropriate FAISS package:
    ```
    # Embeddings and Vector Search
    sentence-transformers==2.2.2 # Or a more recent stable version
    faiss-cpu==1.7.4         # For CPU-only, widely compatible
    # OR
    # faiss-gpu==1.7.4         # If a CUDA-enabled GPU is available and configured
    ```
    (Note: AI should verify current stable versions of sentence-transformers and FAISS. FAISS GPU installation can be complex and depends on CUDA versions.)
**Deliverables:**
*   `requirements.txt` updated with `sentence-transformers` and the chosen `faiss` package.
**Rules/Best Practices:**
*   Start with `faiss-cpu` for broader compatibility in the MVP. GPU can be an optimization if readily available.
*   Ensure these are installed in the virtual environment (`pip install -r requirements.txt`).
---

### Task 2.2: Implement Invoice-to-Text and Embedding Functions (`src/duplicate_detector.py`)
**Context:** To find semantic duplicates, we first need to convert structured invoice data into a textual representation and then into dense vector embeddings.
**Objective:** In `src/duplicate_detector.py`, implement `invoice_to_text_representation(invoice_data_dict)` and `get_embedding(text_string)`.
**Key Files/Modules:** `src/duplicate_detector.py`.
**Detailed Steps/Requirements:**
1.  **File Creation/Update:** Create or open `src/duplicate_detector.py`.
2.  **Import Libraries:** `from sentence_transformers import SentenceTransformer`.
3.  **Initialize Embedding Model:**
    *   Load a pre-trained model, e.g., `model = SentenceTransformer('all-MiniLM-L6-v2')`. This can be a global variable or initialized in a class.
4.  **Implement `invoice_to_text_representation(invoice_data_dict: dict) -> str` (Sub-task 2.2.1):**
    *   **Input:** A dictionary containing extracted invoice fields (e.g., `invoice_no`, `date`, `vendor`, `total_amount`).
    *   **Logic:** Concatenate key fields into a single descriptive string. Consider which fields are most indicative of a unique invoice.
        *   Example: `f"Vendor: {invoice_data_dict.get('vendor', '')} Invoice Number: {invoice_data_dict.get('invoice_no', '')} Date: {invoice_data_dict.get('date', '')} Amount: {invoice_data_dict.get('total_amount', '')}"`
    *   Handle missing fields gracefully (e.g., by using empty strings or placeholders).
    *   **Output:** A single string representation of the invoice.
5.  **Implement `get_embedding(text_string: str) -> list[float]` (Task 2.3 in MVP Plan):**
    *   **Input:** A text string (output from the function above).
    *   **Logic:** Use the initialized `SentenceTransformer` model to encode the text: `embedding = model.encode(text_string)`.
    *   Convert the resulting NumPy array to a Python list: `return embedding.tolist()`.
    *   **Output:** A list of floats representing the dense vector embedding.

**Deliverables:**
*   `src/duplicate_detector.py` with the implemented `invoice_to_text_representation` and `get_embedding` functions.
*   Functions should be testable.
**Rules/Best Practices:**
*   The quality of the text representation directly impacts embedding quality. Experiment if initial results are poor.
*   The embedding model should be loaded once if possible to save time, rather than on every call to `get_embedding`.
---

### Task 2.3 (was 2.4 in MVP plan): Implement FAISS Index Management (`src/duplicate_detector.py`)
**Context:** We need functions to build a FAISS index from our invoice embeddings and to search this index.
**Objective:** In `src/duplicate_detector.py`, implement `build_faiss_index(embeddings_list)` and `search_faiss_index(index, query_embedding, top_k)`.
**Key Files/Modules:** `src/duplicate_detector.py`.
**Detailed Steps/Requirements:**
1.  **Import Libraries:** `import faiss`, `import numpy as np`.
2.  **Implement `build_faiss_index(embeddings_list: list[list[float]]) -> faiss.Index` (Sub-task 2.4.1-2.4.3 in MVP plan):**
    *   **Input:** A list of invoice embeddings (each embedding is a list of floats).
    *   **Convert to NumPy array:** `embeddings_np = np.array(embeddings_list).astype('float32')`.
    *   **Get Dimension:** `dimension = embeddings_np.shape[1]`.
    *   **Initialize FAISS Index:** `index = faiss.IndexFlatIP(dimension)` (Inner Product for cosine similarity). Other index types like `IndexFlatL2` could be used if embeddings aren't normalized and L2 distance is preferred.
    *   **Normalize Embeddings (Important for IndexFlatIP if seeking cosine similarity):** `faiss.normalize_L2(embeddings_np)`.
    *   **Add Embeddings to Index:** `index.add(embeddings_np)`.
    *   **Output:** The populated FAISS index object.
3.  **Implement `search_faiss_index(index: faiss.Index, query_embedding: list[float], top_k: int) -> tuple[np.ndarray, np.ndarray]` (Task 2.5 in MVP Plan):**
    *   **Input:** The FAISS index, a single query embedding (list of floats), and the number of nearest neighbors to retrieve (`top_k`).
    *   **Convert query to NumPy array:** `query_np = np.array([query_embedding]).astype('float32')`.
    *   **Normalize Query Embedding (must match normalization at index time):** `faiss.normalize_L2(query_np)`.
    *   **Search Index:** `distances, indices = index.search(query_np, top_k)`.
    *   **Output:** A tuple containing two NumPy arrays: `distances` (similarity scores or distances) and `indices` (indices of the found embeddings in the original list used to build the index).

**Deliverables:**
*   `src/duplicate_detector.py` updated with the implemented `build_faiss_index` and `search_faiss_index` functions.
**Rules/Best Practices:**
*   Ensure that embeddings are normalized *before* adding to `IndexFlatIP` and *before* searching if cosine similarity is the goal. The scores returned by `IndexFlatIP` on normalized vectors are actual cosine similarities.
*   Handle the case of an empty `embeddings_list` gracefully in `build_faiss_index`.
---

### Task 2.4 (was 2.6 & 2.7 in MVP plan): Implement Duplicate Detection Logic & (Optional) Simple API Endpoint
**Context:** Combine embedding generation, FAISS search, and business heuristics to identify potential duplicate invoices.
**Objective:** Implement `find_potential_duplicates(...)` in `src/duplicate_detector.py`. Optionally, create a very simple local API endpoint (e.g., in `app.py` or a new `api.py`) for testing this.
**Key Files/Modules:** `src/duplicate_detector.py`, `app.py` (optional for API).
**Detailed Steps/Requirements:**
1.  **Import Libraries (in `src/duplicate_detector.py`):** `datetime` (for date comparisons).
2.  **Implement `find_potential_duplicates(target_invoice_id: str, all_invoices_data: list[dict], invoice_embeddings: list[list[float]], faiss_index: faiss.Index, embedding_id_map: dict) -> list[dict]`:**
    *   **Inputs:**
        *   `target_invoice_id`: The ID/filename of the invoice to check for duplicates.
        *   `all_invoices_data`: A list of all invoice dictionaries (from `extracted_invoices.csv`).
        *   `invoice_embeddings`: A list of all invoice embeddings (parallel to `all_invoices_data`).
        *   `faiss_index`: The pre-built FAISS index of `invoice_embeddings`.
        *   `embedding_id_map`: A dictionary mapping the FAISS index (0 to N-1) back to original `invoice_id` or filename. This is crucial if FAISS indices don't directly correspond to a findable ID.
            *   *Alternative to `embedding_id_map`*: The `faiss_index.search` returns indices that directly map to the order in which embeddings were added. If `all_invoices_data` and `invoice_embeddings` maintain this order, the returned indices can be used to look up info in `all_invoices_data`.
    *   **Logic (Sub-task 2.6.1-2.6.4):**
        *   Find the `target_invoice_data` and its `query_embedding` from the inputs based on `target_invoice_id`.
        *   Use `search_faiss_index` to get, e.g., `top_k=5` potential matches.
        *   Initialize `potential_duplicates = []`.
        *   Iterate through the FAISS search results (`distances`, `indices`):
            *   For each `found_idx` and `similarity_score`:
                *   Get the `matched_invoice_data` from `all_invoices_data[found_idx]`.
                *   Skip if `matched_invoice_data['id']` (or filename) is the same as `target_invoice_id`.
                *   **Apply Heuristics (Sub-task 2.6.3):**
                    *   `cosine_similarity_check = similarity_score > 0.9` (Note: `IndexFlatIP` returns cosine similarity directly for normalized vectors).
                    *   Convert `total_amount` for both invoices to float. `amount_difference_check = abs(target_invoice_data['total_amount'] - matched_invoice_data['total_amount']) < 1.0`.
                    *   Parse dates (e.g., `datetime.strptime(date_str, '%Y-%m-%d')`). Handle potential `ValueError` if date formats vary. `date_difference_days_check = abs((target_date_obj - matched_date_obj).days) <= 7`.
                    *   If all heuristic checks pass: Add `{'matched_invoice_id': matched_invoice_data['id'], 'similarity': similarity_score, 'details': matched_invoice_data}` to `potential_duplicates`.
    *   **Output:** `potential_duplicates` list.
3.  **(Optional) Simple Test API Endpoint (Sub-task 2.7):**
    *   If desired for quick testing (before full Streamlit UI), add a route to `app.py` (if using Streamlit as a web server with extensions) or a new `api.py` (using Flask/FastAPI).
    *   This endpoint would: 
        1. Load `data/extracted_invoices.csv` into `all_invoices_data`.
        2. Generate text representations for all invoices.
        3. Generate embeddings for all: `invoice_embeddings_list = [get_embedding(text) for text in all_invoice_texts]`.
        4. Build `faiss_index = build_faiss_index(invoice_embeddings_list)`.
        5. Create the `embedding_id_map` (e.g., ` {i: invoice['filename'] for i, invoice in enumerate(all_invoices_data)}`).
        6. Accept a `target_invoice_id` as a query parameter.
        7. Call `find_potential_duplicates(...)` and return the results as JSON.
    *   *Note:* For MVP, loading and indexing on each API call is acceptable. For production, this would be pre-computed.

**Deliverables:**
*   `src/duplicate_detector.py` updated with `find_potential_duplicates`.
*   (Optional) A simple, working API endpoint for testing duplicate detection.
**Rules/Best Practices:**
*   Make heuristic thresholds (similarity, amount diff, date diff) configurable if possible (e.g., as function arguments or from `config.py`).
*   Thoroughly handle potential errors in data type conversions (amount, date) during heuristic checks.
*   Ensure the mapping between FAISS indices and actual invoice identifiers is correct.
---

### Task 2.5 (was 2.8 & 2.9 in MVP plan): Create Evaluation Data & Script for Duplicates
**Context:** To measure the effectiveness of our duplicate detection logic, we need a ground truth dataset and an evaluation script.
**Objective:** Create `data/duplicate_pairs_ground_truth.csv` and `scripts/evaluate_duplicates.py` to calculate precision, recall, and F1-score for the duplicate detection.
**Key Files/Modules:** `data/duplicate_pairs_ground_truth.csv` (to be created), `scripts/evaluate_duplicates.py` (to be created), `src/duplicate_detector.py`, `data/extracted_invoices.csv`.
**Detailed Steps/Requirements:**
1.  **Create Ground Truth CSV (Sub-task 2.8):**
    *   Create `data/duplicate_pairs_ground_truth.csv`.
    *   Columns: `invoice_id_1`, `invoice_id_2`, `is_duplicate` (1 for true duplicate, 0 for false).
    *   Populate with at least 40 pairs of invoice IDs (filenames from `data/extracted_invoices.csv`).
        *   Include actual duplicates (can be synthetically created by slightly altering an extracted invoice and re-saving it under a new name, then re-extracting to get its own entry in `extracted_invoices.csv`).
        *   Include non-duplicate pairs that might be somewhat similar to test false positives.
2.  **Create Evaluation Script (`scripts/evaluate_duplicates.py`) (Sub-task 2.9):**
    *   **Import Libraries:** `pandas`, `sklearn.metrics` (for `precision_score`, `recall_score`, `f1_score`).
    *   **Import Custom Modules:** `from src.duplicate_detector import get_embedding, build_faiss_index, find_potential_duplicates, invoice_to_text_representation` (and any other necessary setup functions from there).
    *   **Load Data (Sub-task 2.9.1):**
        *   Load `data/extracted_invoices.csv` into a Pandas DataFrame (`all_invoices_df`). Convert to list of dicts if your functions expect that.
        *   Load `data/duplicate_pairs_ground_truth.csv` into a DataFrame (`ground_truth_df`).
    *   **Prepare Embeddings and FAISS Index:**
        *   Generate text representations for all invoices in `all_invoices_df`.
        *   Generate embeddings for all: `invoice_embeddings_list`.
        *   Build the `faiss_index` from these embeddings.
        *   Create the `embedding_id_map` or ensure ordered list access.
    *   **Iterate and Predict (Sub-task 2.9.2):**
        *   Initialize `y_true = []` and `y_pred = []`.
        *   For each row in `ground_truth_df`:
            *   Get `invoice_id_1`, `invoice_id_2`, and `is_duplicate_actual`.
            *   Call `potential_duplicates = find_potential_duplicates(invoice_id_1, all_invoices_data_list, invoice_embeddings_list, faiss_index, embedding_id_map)`.
            *   Check if `invoice_id_2` is present in the `potential_duplicates` returned for `invoice_id_1`.
            *   `predicted_as_duplicate = 1` if found, `0` otherwise.
            *   Append `is_duplicate_actual` to `y_true`.
            *   Append `predicted_as_duplicate` to `y_pred`.
    *   **Calculate Metrics (Sub-task 2.9.3):**
        *   `precision = precision_score(y_true, y_pred)`
        *   `recall = recall_score(y_true, y_pred)`
        *   `f1 = f1_score(y_true, y_pred)`
        *   Print these scores. Also, consider printing a confusion matrix from `sklearn.metrics`.
3.  **Script Execution:** Make the script runnable from the command line.

**Deliverables:**
*   `data/duplicate_pairs_ground_truth.csv` file with at least 40 labeled pairs.
*   `scripts/evaluate_duplicates.py` script that calculates and prints precision, recall, and F1-score.
*   A log or screenshot of the evaluation script output showing the metrics (target F1 â‰¥ 0.85).
**Rules/Best Practices:**
*   The ground truth data should be carefully curated and representative of expected scenarios.
*   Ensure the evaluation correctly maps predictions back to the specific pairs in the ground truth.
*   If `find_potential_duplicates` returns multiple items, the evaluation needs to check if the *specific* paired invoice (`invoice_id_2`) is among them.
--- 

## Sprint 3: Graph & Basic Compliance Demo

### Task 3.1: Update Dependencies in `requirements.txt`
**Context:** We need modules for Neo4j connectivity and GNN prototyping.
**Objective:** Add packages `py2neo`, `torch`, `torch-geometric`, `matplotlib`, `scikit-learn`, and `networkx`.
**Key Files/Modules:** `requirements.txt`.
**Detailed Steps/Requirements:**
1. Open `requirements.txt`.
2. Append:
   ```
   # Graph & GNN
   py2neo==2021.2.3
   torch>=2.0.1
   torch-scatter>=latest      # as required by torch-geometric
   torch-sparse>=latest       # as required by torch-geometric
   torch-cluster>=latest      # as required by torch-geometric
   torch-spline-conv>=latest  # as required by torch-geometric
   torch-geometric>=2.4.0

   # Visualization & evaluation
   matplotlib>=3.5.0
   scikit-learn>=1.2.0
   networkx>=2.8.0
   ```
**Deliverables:** Updated `requirements.txt` with the above lines.
**Rules/Best Practices:** Pin or specify minimum versions. Ensure compatibility between `torch` and `torch-geometric`.
---

### Task 3.2: Create `src/graph_manager.py` with Neo4j Utilities
**Context:** We need a central module for managing Neo4j interactions.
**Objective:** Implement functions: `connect_to_neo4j()`, `clear_graph(graph)`, `load_invoices_to_graph(graph, invoices_df)`, `load_contracts_to_graph(graph, contracts_df)`.
**Key Files/Modules:** `src/graph_manager.py`.
**Detailed Steps/Requirements:**
1. Create `src/graph_manager.py`.
2. Import:
   ```python
   from py2neo import Graph, Node, Relationship
   import pandas as pd
   from src.config import NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD
   ```
3. Implement `def connect_to_neo4j():` returning a `Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))`.
4. Implement `def clear_graph(graph): graph.run("MATCH (n) DETACH DELETE n")`.
5. Implement `def load_invoices_to_graph(graph, invoices_df):`
   * Iterate rows, create `Node("Invoice", invoice_id=..., vendor=..., date=..., total_amount=...)`.
   * Merge Vendor nodes: `Node("Vendor", name=vendor_name)` via `graph.merge(...)`.
   * Create `Relationship(invoice_node, "ISSUED_BY", vendor_node)`.
6. Implement `def load_contracts_to_graph(graph, contracts_df):`
   * Iterate rows, create `Node("Contract", contract_id=..., vendor=..., max_value=..., start_date=..., end_date=...)`.
   * Merge Vendor nodes, create `Relationship(contract_node, "HAS_CONTRACT", vendor_node)`.

**Deliverables:** `src/graph_manager.py` with all above functions.
**Rules:** Use `graph.merge` for idempotency. Validate DataFrame structure.
---

### Task 3.3: Script to Populate Neo4j (`scripts/populate_neo4j.py`)
**Context:** Automate loading invoices and contracts into Neo4j.
**Objective:** Create a script to connect to Neo4j, clear, and load data.
**Key Files/Modules:** `scripts/populate_neo4j.py`, `src/graph_manager.py`, `data/extracted_invoices.csv`, `data/contracts.csv`.
**Detailed Steps/Requirements:**
1. Create `scripts/populate_neo4j.py`.
2. Import `pandas`, `connect_to_neo4j`, `clear_graph`, `load_invoices_to_graph`, `load_contracts_to_graph`.
3. Load `invoices_df = pd.read_csv('data/extracted_invoices.csv')`, `contracts_df = pd.read_csv('data/contracts.csv')`.
4. Call `graph = connect_to_neo4j()`, then `clear_graph(graph)`, `load_invoices_to_graph(graph, invoices_df)`, `load_contracts_to_graph(graph, contracts_df)`.
5. Print summary counts (nodes loaded).

**Deliverables:** `scripts/populate_neo4j.py` script.
**Rules:** Ensure CSV schema matches loader functions.
---

### Task 3.4: Implement Basic Compliance Rule Function (`src/graph_manager.py`)
**Context:** Validate invoice amounts against contract maximums via a Cypher query.
**Objective:** Add `def check_invoice_exceeds_contract(graph, invoice_id):` that returns records where invoice > contract.max_value.
**Detailed Steps/Requirements:**
1. In `src/graph_manager.py`, implement:
   ```python
   def check_invoice_exceeds_contract(graph, invoice_id):
       query = '''
       MATCH (i:Invoice {invoice_id: $invoice_id})-[:ISSUED_BY]->(:Vendor)<-[:HAS_CONTRACT]-(c:Contract)
       WHERE i.total_amount > c.max_value
       RETURN i.invoice_id AS invoice_id, i.total_amount AS invoice_amount,
              c.contract_id AS contract_id, c.max_value AS contract_value
       '''
       return graph.run(query, invoice_id=invoice_id).data()
   ```
2. Test manually by calling the function in REPL or small script.

**Deliverables:** Updated `src/graph_manager.py` with `check_invoice_exceeds_contract`.
**Rules:** Use parameterized Cypher to prevent injection.
---

### Task 3.5: Create GNN Compliance Model (`src/gnn_compliance_model.py`)
**Context:** Prototype a small Graph Neural Network on our Neo4j-exported graph.
**Objective:** Implement `InvoiceGNN` class, `export_data_for_gnn(graph)`, `train_gnn(model, data, epochs)`, and `predict_compliance_with_gnn(model, graph_data, invoice_id)`.
**Key Files/Modules:** `src/gnn_compliance_model.py`.
**Detailed Steps/Requirements:**
1. Create `src/gnn_compliance_model.py`.
2. Import:
   ```python
   import torch
   import torch.nn.functional as F
   from torch_geometric.nn import GraphSAGE
   from torch_geometric.data import Data
   from src.graph_manager import connect_to_neo4j
   ```
3. Define `class InvoiceGNN(torch.nn.Module):` with two `GraphSAGE` layers and a `Linear` output.
4. Implement `export_data_for_gnn(graph):`
   * Run Cypher to fetch nodes with feature vectors and edge lists.
   * Build `x`, `edge_index`, `y`, `train_mask`, `val_mask`, `test_mask` tensors.
   * Return a PyG `Data` object or dict.
5. Implement `train_gnn(model, data, epochs):` training loop using `BCEWithLogitsLoss` and Adam optimizer.
6. Implement `predict_compliance_with_gnn(model, graph_data, invoice_id):` extract node embedding or output for specific invoice node and return probability.

**Deliverables:** `src/gnn_compliance_model.py` with model and helper functions.
**Rules:** Keep graph small (<500 nodes) for CPU training. Use masks.
---

### Task 3.6: GNN Training & Visualization Script (`scripts/train_gnn_demo.py`)
**Context:** Run the GNN prototype and visualize learned embeddings.
**Objective:** Create a script to load graph data, train the model, report metrics, and optionally plot t-SNE embeddings.
**Key Files/Modules:** `scripts/train_gnn_demo.py`, `src/gnn_compliance_model.py`.
**Detailed Steps/Requirements:**
1. Create `scripts/train_gnn_demo.py`.
2. Import `connect_to_neo4j`, `export_data_for_gnn`, `InvoiceGNN`, `train_gnn`, plus `matplotlib`, `sklearn.manifold.TSNE`, and `torch`.
3. Connect to Neo4j and `data = export_data_for_gnn(graph)`.
4. Instantiate `model = InvoiceGNN(data.num_features)`.
5. Call `trained_model = train_gnn(model, data, epochs=100)`.
6. Print training and validation accuracy/loss.
7. (Optional) Extract node embeddings from `trained_model`, run `TSNE(n_components=2)`, and save `gnn_embeddings.png`.
8. Save model state dict to `trained_gnn_model.pt`.

**Deliverables:**
* `scripts/train_gnn_demo.py` script.
* Terminal output showing training metrics.
* `gnn_embeddings.png` visualization (optional).
* `trained_gnn_model.pt` file.
**Rules:** Use CPU; GPU optional if available.
---

## Sprint 4: Streamlit UI & Demo Preparation

### Task 4.1: Streamlit UI - File Upload & Extraction Display
**Context:** Users should be able to upload an invoice and immediately see the extracted fields.
**Objective:** Enhance `app.py` to accept PDF/image uploads, call the vision extractor, and render results in the UI.
**Key Files/Modules:** `app.py`, `src/vision_extractor.py`, `src/config.py`.
**Detailed Steps/Requirements:**
1. Import `st` from `streamlit`, `extract_invoice_data` from `src.vision_extractor`, and `VISION_API_KEY` from `src.config`.
2. Add `uploaded_file = st.file_uploader("Upload Invoice (PDF/Image)", type=["pdf","png","jpg","jpeg"])` at the top of the main page.
3. On file upload (`if uploaded_file is not None:`):
   * Read bytes: `image_bytes = uploaded_file.read()`.
   * Display a spinner: `with st.spinner("Extracting fields..."):`.
   * Call `data = extract_invoice_data(image_bytes, VISION_API_KEY)`.
   * If `data` is truthy, convert to DataFrame or dict and render via `st.table(data)` or `st.json(data)`.
   * If `data` is falsy, use `st.error("Extraction failed. See logs.")`.
4. Ensure errors are caught so the UI doesn't crash.

**Deliverables:**
* Updated `app.py` with a working file uploader and extraction display.
* Manual test: drag an invoice file and see a table or JSON of fields.
**Rules/Best Practices:**
* Use `st.spinner` and `st.success`/`st.error` for user feedback.
* Keep layout responsive (wide mode). Use `st.set_page_config(layout="wide")`.
---

### Task 4.2: Streamlit UI - Duplicate Detection Display
**Context:** After extraction, we need to show any potential duplicate invoices.
**Objective:** In `app.py`, invoke duplicate detection logic and show warnings.
**Key Files/Modules:** `app.py`, `src/duplicate_detector.py`, `src/config.py`.
**Detailed Steps/Requirements:**
1. Import `invoice_to_text_representation`, `get_embedding`, `build_faiss_index`, `find_potential_duplicates` from `src.duplicate_detector`.
2. After successful extraction, prepare inputs:
   * Load all extracted data from `data/extracted_invoices.csv` (or cache at startup).
   * Build text reps and embeddings for the dataset.
   * Build FAISS index once per session or on-demand.
3. For the newly extracted invoice, call `find_potential_duplicates(...)` with thresholds from config.
4. If duplicates found, display `st.warning(f"Found {len(dups)} potential duplicates:")` and list each with ID, similarity, amount, date.
5. If none, show `st.info("No duplicates detected.")`.

**Deliverables:**
* Updated `app.py` with duplicate detection section.
* UI test showing warnings and lists for a known duplicate.
**Rules:**
* Avoid rebuilding the index on every upload; cache it in session state.
* Use clear labels and sorting by similarity.
---

### Task 4.3: Streamlit UI - Graph & Compliance Info Display
**Context:** We want to demo compliance checking and graph reasoning in the UI.
**Objective:** Add a compliance section showing Neo4j rule results and, optionally, GNN predictions.
**Key Files/Modules:** `app.py`, `src/graph_manager.py`, `src/gnn_compliance_model.py`, `src/config.py`.
**Detailed Steps/Requirements:**
1. Import `connect_to_neo4j`, `check_invoice_exceeds_contract` from `src.graph_manager`.
2. After duplicate check, add a section:
   ```python
   st.subheader("Compliance Check")
   with st.spinner("Checking compliance..."):
       results = check_invoice_exceeds_contract(graph, invoice_id)
   ```
3. If `results` non-empty, use `st.error` to list violations (`invoice_amount` vs `contract_value`).
   Else, use `st.success("Invoice is within contract limits.")`.
4. (Stretch) Import and load a pre-trained `InvoiceGNN` model and call `predict_compliance_with_gnn`, then display probability via `st.metric` or `st.progress` bar.

**Deliverables:**
* Updated `app.py` with a compliance section.
* Manual demo showing both passing and failing cases.
**Rules:**
* Connect to local Neo4j instance using config values.
* Catch and display any DB or model errors without breaking UI.
---

### Task 4.4: Final Demo Preparation
**Context:** We need performance metrics and presentation materials for stakeholders.
**Objective:** Measure and display performance; prepare deck and video.
**Key Files/Modules:** `scripts/batch_extract.py`, `scripts/evaluate_duplicates.py`, UI metrics.
**Detailed Steps/Requirements:**
1. Add timing in `batch_extract.py` to print average extraction time per invoice and total time.
2. In `scripts/evaluate_duplicates.py`, ensure metrics are logged and saved (e.g., in `logs/duplicate_eval.txt`).
3. In the Streamlit UI, add a sidebar section `Performance Metrics` showing:
   * Average extraction latency (ms)
   * Avg duplicate-check latency (ms)
4. Create a 5-slide pitch deck (`4.11_MVP_Demo_Deck.pptx`):
   * Slide 1: MVP architecture and goal
   * Slide 2: Extraction demo screenshot
   * Slide 3: Duplicate detection results & metrics
   * Slide 4: Compliance demo (cypher + GNN)
   * Slide 5: Next steps & ask
5. Record a 2-3 minute demo video (`4.12_MVP_Demo_Video.mp4`) showing the UI flow and highlighting key features.

**Deliverables:**
* Updated `batch_extract.py` and `evaluate_duplicates.py` with logging.
* Performance metrics displayed in UI.
* `4.11_MVP_Demo_Deck.pptx` and `4.12_MVP_Demo_Video.mp4` files.
**Rules:**
* Keep deck concise and visual. Videos should be under 3 minutes.
--- 